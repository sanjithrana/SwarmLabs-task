# SwarmLabs-task
ğŸ§  AI Q&A Agent using LangGraph + RAG

A fully functional Retrieval-Augmented Generation (RAG) agent built using LangGraph and LangChain, capable of answering user questions from a local knowledge base.

This project demonstrates a complete AI agent workflow â€” planning, retrieval, answer generation, and reflection â€” with support for OpenAI, Groq, or Hugging Face models.

ğŸ“‹ Features

âœ… End-to-End RAG Pipeline â€” Retrieve relevant data and generate contextual answers.
âœ… LangGraph Workflow â€” Modular AI agent with plan â†’ retrieve â†’ answer â†’ reflect nodes.
âœ… Multiple LLM Support â€” Works with OpenAI, Groq, or Hugging Face models.
âœ… Vector Database (Chroma) â€” Stores embeddings for fast document retrieval.
âœ… Fallback Mode â€” Automatically switches to a free Hugging Face model if API keys fail.
âœ… Streamlit UI (optional) â€” Interact with the agent using a simple web app.
âœ… Evaluation Module â€” Compute BLEU/ROUGE scores or judge with an LLM.

ğŸ—ï¸ Project Structure
rag_agent/
â”‚
â”œâ”€â”€ app.py                 # Main LangGraph + RAG agent
â”œâ”€â”€ ui.py                  # (Optional) Streamlit interactive UI
â”œâ”€â”€ evaluate_agent.py      # Evaluation module (ROUGE, BLEU, LLM Judge)
â”œâ”€â”€ requirements.txt       # All dependencies
â”œâ”€â”€ README.md              # This file
â””â”€â”€ data/                  # Knowledge base folder
    â”œâ”€â”€ renewable_energy.txt
    â”œâ”€â”€ artificial_intelligence.txt
    â”œâ”€â”€ machine_learning.txt
    â”œâ”€â”€ data_science.txt
    â”œâ”€â”€ ethics_in_ai.txt
    â”œâ”€â”€ cloud_computing.txt
    â””â”€â”€ future_of_ai.txt

âš™ï¸ Setup Instructions
1ï¸âƒ£ Clone or Download the Repository
git clone https://github.com/yourusername/rag_agent.git
cd rag_agent

2ï¸âƒ£ Create and Activate a Virtual Environment
python -m venv venv
venv\Scripts\activate  # (Windows)
source venv/bin/activate  # (Mac/Linux)

3ï¸âƒ£ Install Dependencies
pip install -r requirements.txt

4ï¸âƒ£ Add API Keys

Create a .env file inside rag_agent/ and add your keys:

OPENAI_API_KEY=sk-your_openai_key_here
GROQ_API_KEY=gsk-your_groq_key_here
HUGGINGFACEHUB_API_TOKEN=hf_your_huggingface_token_here


If OpenAI or Groq keys fail, the app automatically switches to Hugging Face.

ğŸš€ Running the Project
â–¶ï¸ Run the RAG Agent (CLI)
python app.py


Then type your question, for example:

What are the benefits of renewable energy?


Example Output:

ğŸ§© PLAN NODE: Deciding if retrieval is needed
ğŸ” RETRIEVE NODE: Fetching relevant documents
ğŸ’¬ ANSWER NODE: Generating answer with LLM
ğŸª REFLECT NODE: Evaluating answer relevance

âœ… Final Answer:
According to the context, the benefits of renewable energy include:
1. Sustainability
2. Energy independence
3. Reduced pollution
4. Job creation

Reflection: YES

ğŸ’» (Optional) Run Streamlit UI
streamlit run ui.py


This opens an interactive Q&A web interface.

ğŸ§¾ Evaluation (Optional)

Run the evaluation script to measure RAG quality:

python evaluate_agent.py


It computes:

ROUGE Score

BLEU Score

LLM-as-a-Judge (optional GPT evaluation)

ğŸ§© Key Components
Node	Function
Plan	Understands user intent and decides if retrieval is needed.
Retrieve	Fetches relevant documents from Chroma vector store.
Answer	Generates a contextual answer using the LLM.
Reflect	Evaluates answer completeness and relevance.
ğŸ“š Technologies Used
Category	Tools
Framework	LangGraph, LangChain
Vector DB	ChromaDB
Embeddings	Hugging Face (sentence-transformers/all-MiniLM-L6-v2)
LLMs	OpenAI / Groq / Hugging Face
Frontend (optional)	Streamlit
Evaluation	ROUGE, BLEU, LLM-as-Judge
ğŸ§  Example Knowledge Base Topics

Renewable Energy

Artificial Intelligence

Machine Learning

Data Science

Cloud Computing

AI Ethics

Future of AI

ğŸ› ï¸ Troubleshooting
Issue	Cause	Solution
Invalid API Key	Wrong or missing .env key	Update .env with valid API key
RateLimitError	Free API quota used up	Switch to Groq or HuggingFace model
FileNotFoundError: data	Missing folder	Create /data/ and add .txt files
_thread.RLock warning	Windows multiprocessing issue	Run pip install -U multiprocess dill
JSON output from LLM	LLM returned object	Fixed via .content extraction in code
ğŸ§° Requirements

See requirements.txt for the complete list:

langchain
langchain-core
langchain-community
langchain-openai
langchain-huggingface
langchain-text-splitters
langgraph
chromadb
sentence-transformers
tiktoken
openai
python-dotenv
streamlit
evaluate
transformers
multiprocess
dill


Install with:

pip install -r requirements.txt

ğŸ’¡ Future Enhancements

Add memory for multi-turn conversations.

Integrate LangSmith or TruLens for trace logging.

Add web-based document upload for custom RAG context.

Use local models (Llama 3, Mistral) for offline usage.

ğŸ§‘â€ğŸ’» Author

Developed by: sanjith
Contact: your.chilupurisanjith18@gmail.com


GitHub: github.com/yourusername
